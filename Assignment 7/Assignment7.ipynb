{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "585670ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/TE/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/TE/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/TE/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloaded Packages\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "24399145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 'nltk'\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b109411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = '''Data science is an interdisciplinary academic field that uses statistics, scientific computing, scientific methods, processes, algorithms and systems to extract or extrapolate knowledge and insights from noisy,structured, and unstructured data. Data science also integrates domain knowledge from the underlying applicationdomain (e.g., natural sciences, information technology, and medicine). Data science is multifaceted and can bedescribed as a science, a research paradigm, a research method, a discipline, a workflow, and a profession.'''\n",
    "\n",
    "text2 = '''Data science is the study of data to extract meaningful insights for business. It is a multidisciplinary approach that combines principles and practices from the fields of mathematics, statistics, artificial intelligence, and computer engineering to analyze large amounts of data.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "678f7480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: \n",
      "data science is an interdisciplinary academic field that uses statistics scientific computing scientific methods processes algorithms and systems to extract or extrapolate knowledge and insights from noisystructured and unstructured data data science also integrates domain knowledge from the underlying applicationdomain eg natural sciences information technology and medicine data science is multifaceted and can bedescribed as a science a research paradigm a research method a discipline a workflow and a profession\n",
      "\n",
      "Text 2: \n",
      "data science is the study of data to extract meaningful insights for business it is a multidisciplinary approach that combines principles and practices from the fields of mathematics statistics artificial intelligence and computer engineering to analyze large amounts of data\n",
      "\n",
      "Text 1 after Punctuation Removal: \n",
      "data science is an interdisciplinary academic field that uses statistics scientific computing scientific methods processes algorithms and systems to extract or extrapolate knowledge and insights from noisystructured and unstructured data data science also integrates domain knowledge from the underlying applicationdomain eg natural sciences information technology and medicine data science is multifaceted and can bedescribed as a science a research paradigm a research method a discipline a workflow and a profession\n",
      "\n",
      "Text 2 after Punctuation Removal: \n",
      "data science is the study of data to extract meaningful insights for business it is a multidisciplinary approach that combines principles and practices from the fields of mathematics statistics artificial intelligence and computer engineering to analyze large amounts of data\n"
     ]
    }
   ],
   "source": [
    "print(\"Text 1: \")\n",
    "print(text1)\n",
    "print(\"\\nText 2: \")\n",
    "print(text2)\n",
    "\n",
    "text1 = text1.lower()\n",
    "text2 = text2.lower()\n",
    "text1 = text1.translate(str.maketrans('','',string.punctuation))\n",
    "text2 = text2.translate(str.maketrans('','',string.punctuation))\n",
    "\n",
    "print(\"\\nText 1 after Punctuation Removal: \")\n",
    "print(text1)\n",
    "print(\"\\nText 2 after Punctuation Removal: \")\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5438361b",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "abd356c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens1 = nltk.word_tokenize(text1)\n",
    "tokens2 = nltk.word_tokenize(text2)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2cc11de7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenization of text 1: \n",
      "['data', 'science', 'is', 'an', 'interdisciplinary', 'academic', 'field', 'that', 'uses', 'statistics', 'scientific', 'computing', 'scientific', 'methods', 'processes', 'algorithms', 'and', 'systems', 'to', 'extract', 'or', 'extrapolate', 'knowledge', 'and', 'insights', 'from', 'noisystructured', 'and', 'unstructured', 'data', 'data', 'science', 'also', 'integrates', 'domain', 'knowledge', 'from', 'the', 'underlying', 'applicationdomain', 'eg', 'natural', 'sciences', 'information', 'technology', 'and', 'medicine', 'data', 'science', 'is', 'multifaceted', 'and', 'can', 'bedescribed', 'as', 'a', 'science', 'a', 'research', 'paradigm', 'a', 'research', 'method', 'a', 'discipline', 'a', 'workflow', 'and', 'a', 'profession']\n",
      "\n",
      "Word Tokenization of text 2: \n",
      "['data', 'science', 'is', 'the', 'study', 'of', 'data', 'to', 'extract', 'meaningful', 'insights', 'for', 'business', 'it', 'is', 'a', 'multidisciplinary', 'approach', 'that', 'combines', 'principles', 'and', 'practices', 'from', 'the', 'fields', 'of', 'mathematics', 'statistics', 'artificial', 'intelligence', 'and', 'computer', 'engineering', 'to', 'analyze', 'large', 'amounts', 'of', 'data']\n"
     ]
    }
   ],
   "source": [
    "print(\"Word Tokenization of text 1: \")\n",
    "print(tokens1)\n",
    "print(\"\\nWord Tokenization of text 2: \")\n",
    "print(tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d1d3a33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens1sent = nltk.sent_tokenize(text1)\n",
    "tokens2sent = nltk.sent_tokenize(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2cd3e40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokenization of text 1: \n",
      "['data science is an interdisciplinary academic field that uses statistics scientific computing scientific methods processes algorithms and systems to extract or extrapolate knowledge and insights from noisystructured and unstructured data data science also integrates domain knowledge from the underlying applicationdomain eg natural sciences information technology and medicine data science is multifaceted and can bedescribed as a science a research paradigm a research method a discipline a workflow and a profession']\n",
      "\n",
      "Sentence Tokenization of text 2: \n",
      "['data science is the study of data to extract meaningful insights for business it is a multidisciplinary approach that combines principles and practices from the fields of mathematics statistics artificial intelligence and computer engineering to analyze large amounts of data']\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence Tokenization of text 1: \")\n",
    "print(tokens1sent)\n",
    "print(\"\\nSentence Tokenization of text 2: \")\n",
    "print(tokens2sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c897857",
   "metadata": {},
   "source": [
    "## POS Tagging\n",
    "\n",
    "#### Abbrevation Meaning\n",
    "JJ    -    adjective – ‘big’ \\\n",
    "JJR   -    adjective, comparative – ‘bigger’ \\\n",
    "JJS   -    adjective, superlative – ‘biggest’ \\\n",
    "NN    -    noun, singular ‘- desk’ \\\n",
    "NNS   -    noun plural – ‘desks’ \\\n",
    "NNP   -    proper noun, singular – ‘Harrison’ \\\n",
    "NNPS  -    proper noun, plural – ‘Americans’ \\\n",
    "RB    -    adverb – very, silently, \\\n",
    "RBR   -    adverb, comparative – better \\\n",
    "RBS   -    adverb, superlative – best \\\n",
    "RP    -    particle – give up \\\n",
    "VB    -    verb, base form – take \\\n",
    "VBD   -    verb, past tense – took \\\n",
    "VBG   -    verb, gerund/present participle – taking \\\n",
    "VBN   -    verb, past participle – taken \\\n",
    "VBP   -    verb, sing. present, non-3d – take \\\n",
    "VBZ   -    verb, 3rd person sing. present – takes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3d7ea7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'up', 'mightn', 'too', 'their', 's', 'after', 'the', 'll', 'it', 'hasn', 'those', \"you're\", \"mightn't\", 'myself', 'shouldn', 'in', 'yours', 'd', 'weren', 'did', 'but', 'can', 'how', \"won't\", 'i', 'am', 'won', 'mustn', 'most', 'have', 'are', 'them', 'be', 'do', 'both', 'they', 'will', 'o', 'what', \"that'll\", 'nor', 'itself', 'yourselves', \"doesn't\", 'this', \"aren't\", 'doesn', 'hadn', 'wouldn', 'or', 'at', 'other', 'themselves', \"you'd\", 'theirs', 'had', 'isn', 'from', 'same', 've', 'into', 'm', \"shouldn't\", 'these', 'to', 'each', 'haven', 'against', 'which', 'where', \"should've\", 'because', \"haven't\", 'only', 'not', \"you'll\", \"you've\", 'out', 'here', \"it's\", 'when', 'such', 'a', 'about', 'an', \"shan't\", 'above', \"isn't\", \"don't\", 'of', 'once', 'further', \"hasn't\", 'and', 'then', 'aren', 'over', 'with', 'didn', 'very', 'by', 'her', 'for', 'doing', 'we', 'as', 'shan', 'its', 'if', 'whom', 'ma', 'before', 'should', 'between', 'on', 'ourselves', 'hers', 'ain', \"hadn't\", 'was', 'under', 'my', 'all', 'why', 'needn', 'yourself', \"she's\", 'does', \"needn't\", 're', 'more', 'me', 'having', 'couldn', 'few', 'through', 'until', 'no', 'now', 'own', 'that', \"mustn't\", 'who', 'during', 'again', 'so', \"didn't\", 'she', 'don', \"couldn't\", \"weren't\", 'our', \"wouldn't\", 'there', 'being', 'down', 'off', \"wasn't\", 'were', 'than', 'while', 'just', 'has', 't', 'y', 'some', 'any', 'been', 'herself', 'you', 'wasn', 'him', 'your', 'his', 'himself', 'below', 'ours', 'is', 'he'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4403f7ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagging for text 1: \n",
      "{('processes', 'VBZ'), ('multifaceted', 'VBD'), ('scientific', 'JJ'), ('discipline', 'NN'), ('extrapolate', 'JJ'), ('knowledge', 'NN'), ('paradigm', 'NN'), ('underlying', 'VBG'), ('science', 'NN'), ('extract', 'JJ'), ('method', 'NN'), ('eg', 'JJ'), ('unstructured', 'JJ'), ('domain', 'NN'), ('medicine', 'NN'), ('academic', 'JJ'), ('workflow', 'IN'), ('uses', 'VBZ'), ('statistics', 'NNS'), ('technology', 'NN'), ('natural', 'JJ'), ('bedescribed', 'JJ'), ('interdisciplinary', 'JJ'), ('methods', 'NNS'), ('field', 'NN'), ('sciences', 'NNS'), ('data', 'NNS'), ('integrates', 'VBZ'), ('research', 'NN'), ('profession', 'NN'), ('insights', 'NNS'), ('computing', 'VBG'), ('also', 'RB'), ('algorithms', 'JJ'), ('applicationdomain', 'NN'), ('systems', 'NNS'), ('noisystructured', 'VBN'), ('information', 'NN')}\n",
      "\n",
      "POS Tagging for text 2: \n",
      "{('study', 'NN'), ('meaningful', 'JJ'), ('mathematics', 'NNS'), ('computer', 'NN'), ('intelligence', 'NN'), ('artificial', 'JJ'), ('science', 'NN'), ('large', 'JJ'), ('principles', 'NNS'), ('combines', 'NNS'), ('amounts', 'NNS'), ('statistics', 'NNS'), ('extract', 'VBP'), ('fields', 'NNS'), ('multidisciplinary', 'JJ'), ('data', 'NNS'), ('approach', 'NN'), ('insights', 'NNS'), ('analyze', 'JJ'), ('practices', 'NNS'), ('business', 'NN'), ('engineering', 'NN')}\n"
     ]
    }
   ],
   "source": [
    "for i in tokens1:\n",
    "    tokenized = [w for w in tokens1 if not w in stop_words]\n",
    "    tagged1 = set(nltk.pos_tag(tokenized))\n",
    "print(\"POS Tagging for text 1: \")\n",
    "print(tagged1)\n",
    "\n",
    "for i in tokens2:\n",
    "    tokenized = [w for w in tokens2 if not w in stop_words]\n",
    "    tagged2 = set(nltk.pos_tag(tokenized))\n",
    "print(\"\\nPOS Tagging for text 2: \")\n",
    "print(tagged2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d946e9d",
   "metadata": {},
   "source": [
    "## Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "049a4e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stowords Removal for text 1: \n",
      "['data', 'science', 'interdisciplinary', 'academic', 'field', 'uses', 'statistics', 'scientific', 'computing', 'methods', 'processes', 'algorithms', 'systems', 'extract', 'extrapolate', 'knowledge', 'insights', 'noisystructured', 'unstructured', 'also', 'integrates', 'domain', 'underlying', 'applicationdomain', 'eg', 'natural', 'sciences', 'information', 'technology', 'medicine', 'multifaceted', 'bedescribed', 'research', 'paradigm', 'method', 'discipline', 'workflow', 'profession']\n",
      "\n",
      "Stowords Removal for text 2: \n",
      "['data', 'science', 'study', 'extract', 'meaningful', 'insights', 'business', 'multidisciplinary', 'approach', 'combines', 'principles', 'practices', 'fields', 'mathematics', 'statistics', 'artificial', 'intelligence', 'computer', 'engineering', 'analyze', 'large', 'amounts']\n"
     ]
    }
   ],
   "source": [
    "tokens1_after_stopwords_removal = []\n",
    "tokens2_after_stopwords_removal = []\n",
    "\n",
    "for w in tokens1:\n",
    "    if w not in tokens1_after_stopwords_removal:\n",
    "        if w not in stop_words:\n",
    "            tokens1_after_stopwords_removal.append(w)\n",
    "print(\"Stowords Removal for text 1: \")\n",
    "print(tokens1_after_stopwords_removal)\n",
    "\n",
    "for w in tokens2:\n",
    "    if w not in tokens2_after_stopwords_removal:\n",
    "        if w not in stop_words:\n",
    "            tokens2_after_stopwords_removal.append(w)\n",
    "print(\"\\nStowords Removal for text 2: \")\n",
    "print(tokens2_after_stopwords_removal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feecdafd",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6f842ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming for text 1: \n",
      "data  :  data\n",
      "science  :  scienc\n",
      "interdisciplinary  :  interdisciplinari\n",
      "academic  :  academ\n",
      "field  :  field\n",
      "uses  :  use\n",
      "statistics  :  statist\n",
      "scientific  :  scientif\n",
      "computing  :  comput\n",
      "methods  :  method\n",
      "processes  :  process\n",
      "algorithms  :  algorithm\n",
      "systems  :  system\n",
      "extract  :  extract\n",
      "extrapolate  :  extrapol\n",
      "knowledge  :  knowledg\n",
      "insights  :  insight\n",
      "noisystructured  :  noisystructur\n",
      "unstructured  :  unstructur\n",
      "also  :  also\n",
      "integrates  :  integr\n",
      "domain  :  domain\n",
      "underlying  :  underli\n",
      "applicationdomain  :  applicationdomain\n",
      "eg  :  eg\n",
      "natural  :  natur\n",
      "sciences  :  scienc\n",
      "information  :  inform\n",
      "technology  :  technolog\n",
      "medicine  :  medicin\n",
      "multifaceted  :  multifacet\n",
      "bedescribed  :  bedescrib\n",
      "research  :  research\n",
      "paradigm  :  paradigm\n",
      "method  :  method\n",
      "discipline  :  disciplin\n",
      "workflow  :  workflow\n",
      "profession  :  profess\n",
      "\n",
      "Stemming for text 2: \n",
      "data  :  data\n",
      "science  :  scienc\n",
      "study  :  studi\n",
      "extract  :  extract\n",
      "meaningful  :  meaning\n",
      "insights  :  insight\n",
      "business  :  busi\n",
      "multidisciplinary  :  multidisciplinari\n",
      "approach  :  approach\n",
      "combines  :  combin\n",
      "principles  :  principl\n",
      "practices  :  practic\n",
      "fields  :  field\n",
      "mathematics  :  mathemat\n",
      "statistics  :  statist\n",
      "artificial  :  artifici\n",
      "intelligence  :  intellig\n",
      "computer  :  comput\n",
      "engineering  :  engin\n",
      "analyze  :  analyz\n",
      "large  :  larg\n",
      "amounts  :  amount\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "print(\"Stemming for text 1: \")\n",
    "for w in tokens1_after_stopwords_removal:\n",
    "    print(w, \" : \", ps.stem(w))\n",
    "\n",
    "\n",
    "print(\"\\nStemming for text 2: \")\n",
    "for w in tokens2_after_stopwords_removal:\n",
    "    print(w, \" : \", ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d65439b",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3215304c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization for text 1: \n",
      "data  :  data\n",
      "science  :  science\n",
      "interdisciplinary  :  interdisciplinary\n",
      "academic  :  academic\n",
      "field  :  field\n",
      "uses  :  us\n",
      "statistics  :  statistic\n",
      "scientific  :  scientific\n",
      "computing  :  computing\n",
      "methods  :  method\n",
      "processes  :  process\n",
      "algorithms  :  algorithm\n",
      "systems  :  system\n",
      "extract  :  extract\n",
      "extrapolate  :  extrapolate\n",
      "knowledge  :  knowledge\n",
      "insights  :  insight\n",
      "noisystructured  :  noisystructured\n",
      "unstructured  :  unstructured\n",
      "also  :  also\n",
      "integrates  :  integrates\n",
      "domain  :  domain\n",
      "underlying  :  underlying\n",
      "applicationdomain  :  applicationdomain\n",
      "eg  :  eg\n",
      "natural  :  natural\n",
      "sciences  :  science\n",
      "information  :  information\n",
      "technology  :  technology\n",
      "medicine  :  medicine\n",
      "multifaceted  :  multifaceted\n",
      "bedescribed  :  bedescribed\n",
      "research  :  research\n",
      "paradigm  :  paradigm\n",
      "method  :  method\n",
      "discipline  :  discipline\n",
      "workflow  :  workflow\n",
      "profession  :  profession\n",
      "\n",
      "Lemmatization for text 2: \n",
      "data  :  data\n",
      "science  :  science\n",
      "study  :  study\n",
      "extract  :  extract\n",
      "meaningful  :  meaningful\n",
      "insights  :  insight\n",
      "business  :  business\n",
      "multidisciplinary  :  multidisciplinary\n",
      "approach  :  approach\n",
      "combines  :  combine\n",
      "principles  :  principle\n",
      "practices  :  practice\n",
      "fields  :  field\n",
      "mathematics  :  mathematics\n",
      "statistics  :  statistic\n",
      "artificial  :  artificial\n",
      "intelligence  :  intelligence\n",
      "computer  :  computer\n",
      "engineering  :  engineering\n",
      "analyze  :  analyze\n",
      "large  :  large\n",
      "amounts  :  amount\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"Lemmatization for text 1: \")\n",
    "for w in tokens1_after_stopwords_removal:\n",
    "    print(w, \" : \", lemmatizer.lemmatize(w))\n",
    "\n",
    "print(\"\\nLemmatization for text 2: \")\n",
    "for w in tokens2_after_stopwords_removal:\n",
    "    print(w, \" : \", lemmatizer.lemmatize(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906f77ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
